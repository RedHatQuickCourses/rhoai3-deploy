apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: granite3
  namespace: model-deploy-lab
  annotations:
    # "RawDeployment" mode bypasses Serverless to prevent cold starts
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    serviceAccountName: fast-track-sa
    model:
      # The Format must match what the Runtime supports (vLLM)
      modelFormat:
        name: vLLM
      # This name must match the runtime extracted in Step 2
      runtime: vllm-cuda-runtime
      # The path to your model in MinIO (s3://bucket/folder)
      storageUri: s3://models/granite3
      resources:
        requests:
          nvidia.com/gpu: "1"
          memory: "8Gi"
        limits:
          nvidia.com/gpu: "1"
          memory: "10Gi"