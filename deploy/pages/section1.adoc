= Architecture: Intelligent Routing for Model Deployment
:navtitle: Architecture Deep Dive
:toc: macro

// Antora metadata
:page-role: architecture-concept
:description: Technical deep dive into llm-d components, intelligent routing, and KV cache off-loading for all model deployments.

[.lead]
*Understanding the difference between "deploying a model" and "deploying intelligently" is the key to production-grade AI.*

When you deploy a model from your registry, you have a choice: use standard serving (simple, but wasteful) or use **Distributed Inference with `llm-d`** (intelligent, efficient, and production-ready).

This section explains how `llm-d` transforms every model deployment—whether from your private registry or the public catalog—into an intelligent, cache-aware service that maximizes GPU ROI.

== The Core Concept: Intelligent Disaggregation

The `llm-d` architecture follows a "split-plane" design that separates **orchestration** (Control Plane) from **execution** (Data Plane).

* **The Control Plane (The "Traffic Controller"):** The `llm-d` Inference Scheduler. It maintains real-time metadata about all running model pods, tracks KV Cache locations, and makes intelligent routing decisions.
* **The Data Plane (The "Workers"):** The vLLM pods that actually run inference. These are managed by `llm-d` but execute the model computation.

This separation enables capabilities that standard deployments cannot provide: intelligent routing, cache affinity, and resource optimization.

== Component Breakdown

=== 1. The Inference Scheduler (Control Plane)

This is the "brain" of intelligent routing. It runs as a separate service in your cluster and makes routing decisions for every inference request.

* **Technology:** A Kubernetes-native controller that integrates with the Gateway API.
* **Function:** Maintains a real-time view of all vLLM pods, their health, queue depth, and—critically—their KV Cache state.
* **The Intelligence:** When a request arrives, the scheduler:
  * **Filters** unhealthy or overloaded pods.
  * **Scores** remaining pods based on queue length and **KV Cache Affinity** (does this pod already have the conversation context?).
  * **Routes** the request to the optimal pod.

=== 2. The Gateway API Integration

The Gateway API (Envoy) acts as the "front door" for all inference traffic. It integrates with the Inference Scheduler to execute routing decisions.

* **Technology:** OpenShift Service Mesh or kGateway (Gateway API compliant).
* **Function:** Intercepts HTTP requests, queries the scheduler for routing decisions, and forwards traffic to the selected pod.
* **The Flow:**
  1. User sends a request to `/v1/chat/completions`.
  2. Gateway intercepts and queries the scheduler: *"Where should this go?"*
  3. Scheduler responds with a pod address (based on cache affinity and load).
  4. Gateway forwards the request to that specific pod.

=== 3. The vLLM Pods (Data Plane)

These are the actual inference engines. In an `llm-d` deployment, they are organized into pools:

* **Prefill Pods:** Handle the compute-intensive prompt processing. These are optimized for throughput.
* **Decode Pods:** Handle the latency-sensitive token generation. These are optimized for memory and speed.

* **The KV Cache:** Each pod maintains a cache of conversation contexts in GPU memory. The scheduler uses this cache state to make routing decisions.

=== 4. Model Source Flexibility

`llm-d` works with models from any source—your private registry, the public catalog, or Hugging Face. The intelligent routing and cache management apply **regardless of the model source**. Simply provide the model URI in the standard format (e.g., `hf://Qwen/Qwen3-0.6B` or `s3://your-bucket/model-path`).

== The Data Flow: Intelligent Request Lifecycle

Understanding this flow is critical for troubleshooting and optimization. The scheduler makes the decision, but the Gateway moves the data.

[cols="1,4"]
|===
| **Step** | **What Happens**

| **1. Request In**
| A user sends an OpenAI-compatible request (e.g., `/v1/chat/completions`) to the `HTTPRoute` endpoint.

| **2. Gateway Intercepts**
| The Gateway API (Envoy) in `openshift-ingress` receives the traffic and extracts request metadata (conversation ID, model name, etc.).

| **3. Scheduler Query**
| The Gateway forwards the **request metadata** to the Inference Scheduler, effectively asking: *"Where should this go?"*

| **4. Intelligent Decision (Filter & Score)**
| The scheduler executes its routing logic:
* **Filter:** Removes unhealthy, overloaded, or incompatible pods.
* **Score:** Scores remaining pods based on:
  * Queue depth (lower is better).
  * **KV Cache Affinity** (does this pod already hold the conversation context?).
  * Resource availability.

| **5. Routing Instruction**
| The scheduler responds to the Gateway with a specific pod address:
* **Cache Hit (Fast Path):** Routes to the Decode pod that already has the context.
* **Cache Miss (Slow Path):** Routes to a Prefill pod first, then to a Decode pod for generation.

| **6. Execution**
| The Gateway forwards the actual data payload to the chosen pod. The pod generates the response and streams it back through the Gateway to the user.

|===

== The Value: KV Cache Off-Loading

The KV Cache is the "memory" of your inference service. It stores the conversation context (the processed prompt and generated tokens) in GPU memory. `llm-d` manages this cache intelligently:

=== Cache Affinity Routing

When a user continues a conversation, the scheduler checks which pod holds the relevant KV Cache. If found (cache hit), the request is routed directly to that pod, avoiding expensive Prefill computation.

**The Benefit:** 
* **Lower Latency:** Time-To-First-Token (TTFT) is dramatically reduced for cache hits.
* **Lower Cost:** You skip the expensive Prefill phase, using GPU cycles only for generation.
* **Higher Throughput:** More users can be served on the same hardware.

=== Cache Management Across All Deployments

Whether you deploy from your private registry or the public catalog, `llm-d` provides intelligent cache management:

* **Automatic Cache Tracking:** The scheduler maintains a map of cache locations across all pods.
* **Cache-Aware Scaling:** When scaling up, new pods are populated with cache from existing pods when possible.
* **Cache Eviction:** When memory pressure occurs, the scheduler intelligently evicts least-recently-used caches.

**The Win:** Every model deployment benefits from intelligent cache management, maximizing the ROI of your GPU infrastructure.

== System Requirements for the Lab

To build this architecture in the next module, your cluster must meet these minimums:

[cols="1,2,2"]
|===
| **Component** | **Requirement** | **Role**

| **OpenShift AI**
| v3.0
| The Platform

| **OpenShift**
| 4.19+ (4.20 recommended)
| Required for Gateway API support

| **Gateway API**
| Compliant implementation (Service Mesh or kGateway)
| The routing layer

| **Operators**
| LeaderWorkerSet (LWS) Operator
| Manages pod fleets for `llm-d`

| **Hardware**
| Data Center GPUs (H100, A100, L40S)
| The inference engines

| **Compute**
| 2 vCPUs, 4GB RAM (per pod)
| For scheduler and workers
|===

[WARNING]
.Critical: Network Fabric Requirements
====
**Do not underestimate the network.** +
High-performance "East-West" cache sharing requires a low-latency fabric (InfiniBand or RoCE). A standard 10GbE TCP/IP network will be a bottleneck and is **not supported** for advanced features like Mixture-of-Experts (MoE).
====

---
*Now that you understand the blueprint, it is time to deploy.*
