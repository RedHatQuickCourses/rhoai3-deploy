= Lab: Automated Model Serving
:navtitle: Automated Serving
:toc: macro

// Antora metadata
:page-role: Platform Engineer
:description: Deploying LLMs via CLI with specific KV-Cache and VRAM tuning parameters.

[.lead]
*Click-Ops doesn't scale. GitOps does.*

You will deploy the `Granite-2B` model using a parameterized script that explicitly controls the **KV Cache** size.

== Why script the deployment?

When deploying Large Language Models (LLMs), default settings are rarely optimal.
To prevent Out-Of-Memory (OOM) errors on limited GPUs (like A10Gs or T4s), we must explicitly cap the **Context Window**.

* **The Parameter:** `max-model-len`
* **The Value:** `16000` (tokens)
* **The Effect:** This reserves a specific amount of VRAM for the "Key-Value Cache" (Conversation History).
** If set too high (e.g., 32k), the GPU runs out of memory for the weights.
** If set too low (e.g., 2k), the model cannot remember long documents.

== Step 1: Execute the Deployment

We have provided the `serve_model.sh` script in the `deploy/` directory.

. **Run the script:**
+
[source,bash]
----
chmod +x deploy/serve_model.sh
./deploy/serve_model.sh
----

. **Watch the rollout:**
The script creates two Kubernetes objects:
* **ServingRuntime:** Defines the engine (`vLLM`).
* **InferenceService:** Defines the application and applies the arguments (`--max-model-len=16000`).

== Step 2: Validating the Configuration

Once the script reports `âœ… SUCCESS`, let's verify that our tuning parameters actually took effect.

. **Check the Arguments:**
Verify the Pod is running with the 16k limit.
+
[source,bash]
----
oc get pod -l serving.kserve.io/inferenceservice=granite-2b-server \
  -n rhoai-model-registry-lab \
  -o jsonpath='{.items[0].spec.containers[0].args}'
----
* *Expected Output:* You should see `"--max-model-len=16000"` in the list.

. **Test the Endpoint:**
The script provides a curl command upon success. Run it to confirm the model is inferencing.

[source,bash]
----
export URL=$(oc get inferenceservice granite-2b-server -n rhoai-model-registry-lab -o jsonpath='{.status.url}')

curl -k $URL/v1/completions \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "granite-2b-server",
    "prompt": "Explain the importance of caching in one sentence.",
    "max_tokens": 60
  }'
----
