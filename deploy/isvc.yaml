apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: granite-4-micro
  namespace: model-deploy-lab
  annotations:
    openshift.io/display-name: "Granite 4.0 Micro"
    serving.kserve.io/deploymentMode: RawDeployment
    serving.kserve.io/runtime: vllm-cuda-runtime
spec:
  predictor:
    serviceAccountName: granite-sa
    model:
      modelFormat:
        name: vLLM
      runtime: vllm-cuda-runtime
      storageUri: s3://models/granite4

      resources:
        requests:
          cpu: "4"
          memory: "12Gi"
          nvidia.com/gpu: "1"
        limits:
          cpu: "4"
          memory: "12Gi"
          nvidia.com/gpu: "1"

      # --- CHANGE 1: MOUNT THE VOLUME ---
      volumeMounts:
        - name: template-volume
          mountPath: /mnt/template
          readOnly: true

      args:
        - "--model"
        - "/mnt/models" 
        - "--port"
        - "8080"
        - "--max-model-len"
        - "16000"
        - "--gpu-memory-utilization"
        - "0.6"
        - "--distributed-executor-backend"
        - "mp"
        # --- CHANGE 2: ARGUMENT TO USE TEMPLATE ---
        - "--chat-template"
        - "/mnt/template/granite.jinja"

    # --- CHANGE 3: DEFINE THE VOLUME ---
    volumes:
      - name: template-volume
        configMap:
          name: granite-chat-template