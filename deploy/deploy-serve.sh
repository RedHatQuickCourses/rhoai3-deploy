#!/bin/bash

# =================================================================================
# SCRIPT: deploy-serve.sh
# DESCRIPTION: Deploys a vLLM-based InferenceService using Production Best Practices.
#              (Replicates the exact configuration generated by RHOAI UI)
# =================================================================================

set -e

# --- CONFIGURATION ---
NAMESPACE="model-deploy-lab"
MODEL_NAME="granite-4-micro"
SERVICE_ACCOUNT="models-sa"  # The specific identity used by the UI
SECRET_NAME="storage-config" # The secret created by fast-track.sh
MODEL_URI="s3://models/granite4" # Direct URI to the model

echo "üöÄ Deploying Model: $MODEL_NAME"

# ---------------------------------------------------------------------------------
# 1. Security Setup (The "Customer One" Way)
# ---------------------------------------------------------------------------------
echo "‚û§ Configuring Service Account & Permissions..."

# Create the Service Account if it doesn't exist
if ! oc get sa "$SERVICE_ACCOUNT" -n "$NAMESPACE" > /dev/null 2>&1; then
    oc create sa "$SERVICE_ACCOUNT" -n "$NAMESPACE"
    echo "   ‚úî Created Service Account: $SERVICE_ACCOUNT"
else
    echo "   ‚úî Service Account '$SERVICE_ACCOUNT' already exists."
fi

# Link the Secret to the Service Account
# This allows the model to read S3 credentials without exposing them to the whole namespace
oc secrets link "$SERVICE_ACCOUNT" "$SECRET_NAME" -n "$NAMESPACE" --for=pull,mount
echo "   ‚úî Linked secret '$SECRET_NAME' to '$SERVICE_ACCOUNT'"

# ---------------------------------------------------------------------------------
# 2. Define the Serving Runtime
# ---------------------------------------------------------------------------------
echo "‚û§ Configuring vLLM Runtime..."

cat <<EOF | oc apply -n $NAMESPACE -f -
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: vllm-runtime
  annotations:
    openshift.io/display-name: vLLM (NVIDIA GPU)
    opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
spec:
  supportedModelFormats:
    - name: vLLM
      autoSelect: true
  containers:
    - name: kserve-container
      image: quay.io/modh/vllm:rhoai-2.21-cuda
      command: ["python", "-m", "vllm.entrypoints.openai.api_server"]
      args:
        - "--port=8080"
        - "--model=/mnt/models"
        - "--served-model-name={{.Name}}"
      env:
        - name: HF_HOME
          value: /tmp/hf_home
      ports:
        - containerPort: 8080
          protocol: TCP
      resources:
        requests:
          nvidia.com/gpu: "1"
        limits:
          nvidia.com/gpu: "1"
EOF

# ---------------------------------------------------------------------------------
# 3. Deploy the Inference Service
# ---------------------------------------------------------------------------------
echo "‚û§ Creating InferenceService..."

cat <<EOF | oc apply -n $NAMESPACE -f -
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: $MODEL_NAME
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    serviceAccountName: $SERVICE_ACCOUNT  # <--- CRITICAL: Matches the SA we configured
    model:
      modelFormat:
        name: vLLM
      runtime: vllm-runtime
      storageUri: "$MODEL_URI"            # <--- CRITICAL: Direct URI path, no complex key mapping
      
      # üõ†Ô∏è PERFORMANCE TUNING üõ†Ô∏è
      args:
        - "--dtype=float16"
        - "--max-model-len=8192" 
        - "--gpu-memory-utilization=0.90" 
      resources:
        requests:
          cpu: "4"
          memory: "8Gi"
          nvidia.com/gpu: "1"
        limits:
          cpu: "8"
          memory: "16Gi"
          nvidia.com/gpu: "1" 
EOF

# ---------------------------------------------------------------------------------
# 4. Wait for Readiness
# ---------------------------------------------------------------------------------
echo "‚è≥ Deployment submitted. Waiting for Model to Load..."

# Loop to check status
for i in {1..30}; do
  STATUS=$(oc get inferenceservice $MODEL_NAME -n $NAMESPACE -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' 2>/dev/null)
  
  if [ "$STATUS" == "True" ]; then
    URL=$(oc get inferenceservice $MODEL_NAME -n $NAMESPACE -o jsonpath='{.status.url}')
    echo ""
    echo "‚úÖ SUCCESS: Model is Serving!"
    echo "üîó Endpoint: $URL/v1/completions"
    exit 0
  fi
  
  echo -n "."
  sleep 10
done

echo ""
echo "‚ö†Ô∏è  Timeout. Check logs: oc logs -n $NAMESPACE -l serving.kserve.io/inferenceservice=$MODEL_NAME -c kserve-container"