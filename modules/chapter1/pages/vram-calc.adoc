= Sizing Guide: The VRAM Math
:navtitle: Hardware Sizing
:toc: macro

// Antora metadata
:page-role: MLOps Engineer
:description: How to calculate GPU memory requirements including Model Weights and KV Cache.

[.lead]
*Inference fails for one reason: Out of Memory (OOM).*

A common mistake is looking at a model's file size (e.g., "15GB") and assuming it will fit on a 16GB GPU.
It won't.
To size an inference node correctly, you must account for the **model weights** (static) and the **KV cache** (dynamic).

== The Formula


=== Total VRAM = (Weights \times 1.2) + (KV Cache)


=== Part 1: The Static Cost (Model Weights)
This is the "rent" you pay just to turn the model on.

* **Formula:** `Params (Billions) * Size_Per_Param`
* **Size guide:**
** **FP16 (standard):** 2 Bytes per param.
** **FP8 / INT8:** 1 Byte per param.
** **AWQ (INT4):** 0.5 Bytes per param.

.Example: Granite-8B (FP16)
[source,text]
8 Billion * 2 Bytes = 16 GB
Overhead (CUDA Kernels) = +20%
---------------------------
Static Footprint = ~19.2 GB

*Result:* This fits on an **A10G (24GB)**, but fails on a **T4 (16GB)**.

=== Part 2: The Dynamic Cost (KV Cache)
This is the "variable cost" of your user traffic.
Every token generated for every user consumes VRAM. If you run out, the server crashes.

* **Key Variables:**
** **Context Length:** How much history the model remembers (e.g., 4k vs 128k tokens).
** **Batch Size:** How many concurrent users you serve.

* **The Danger Zone:**
A **Granite-8B** model supports a 128k context window.
If you simply deploy it with defaults, vLLM effectively tries to reserve space for that full 128k window.
**128k Context @ FP16 = ~20 GB of VRAM.**

.The OOM Trap
[source,text]

Static Footprint (19.2 GB) + Full Context (20 GB) = 39.2 GB
---
Hardware Available (A10G) = 24 GB
Result: CRASH (OOMKilled)

== The Engineering Fix: Cap the Context

You cannot magically download more VRAM. You must compromise.
By limiting the context window, you reserve room for the model to breathe.

* **The Parameter:** `--max-model-len`
* **The Math:** Reducing context from 128k to **8k** drops the KV cache requirement from ~20GB to **~1.5GB**.

.Optimized Deployment
[source,text]
Static (19.2 GB) + limited cache (1.5 GB) = 20.7 GB
---
Fits safely on 24GB Card.

== Quick Reference: "Will It Fit?"

Assumes **FP16** precision.

[cols="1,1,1,1"]
|===
| Model | Min VRAM (Weights) | Min Card (Idle) | Recommended Card (Production)

| **Granite-2B**
| ~4 GB
| NVIDIA T4 (16GB)
| NVIDIA L4 (24GB)

| **Granite-8B**
| ~16 GB
| NVIDIA A10G (24GB)
| NVIDIA A100 (40GB) or L40S

| **Llama-3-70B**
| ~140 GB
| 2x A100 (80GB)
| 4x A100 or 2x H100
|===

xref:vllm-tuning.adoc[Next: Tuning the Engine]