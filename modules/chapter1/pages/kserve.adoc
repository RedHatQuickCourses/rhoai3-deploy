= KServe for Production LLMs: From Concept to Raw Deployment
:toc: macro
:toc-title: On this page

This guide documents the architectural progression of serving Large Language Models (LLMs) using KServe. It covers the transition from basic serverless concepts to the "RawDeployment" patterns preferred by OpenShift AI (RHOAI) for production workloads.


== 1. KServe

KServe acts as a Kubernetes-native tool designed to solve the "last mile" problem in machine learning: serving models in production.

Think of KServe as a "universal adapter" for your models. Instead of writing custom web servers (like Flask or FastAPI) for every different framework you use, KServe provides a standardized Custom Resource Definition (CRD) called the `InferenceService`.

Its core value proposition handles the infrastructure heavy lifting so engineers can focus on the model:

* **Abstracted Runtimes:** You provide a model URI (e.g., S3), and KServe automatically spins up the correct container (Triton, TorchServe, vLLM).
* **Standardized Protocol:** It uses the V2 Inference Protocol, meaning client code remains consistent even if the backend framework changes.
* **Serverless Capabilities:** Historically, KServe is known for scaling replicas based on traffic, including "scaling to zero" to save costs.

== 2. The Engine: KServe + vLLM

For serving LLMs on Kubernetes, pairing **KServe** with **vLLM** is currently the industry standard.

* **KServe (The Manager):** Handles networking, DNS, and autoscaling.
* **vLLM (The Engine):** Handles the actual math and memory management. It is famous for _PagedAttention_, an algorithm that manages GPU memory much more efficiently than standard PyTorch, allowing for larger request batches.

== 3. Configuration: The Template and The Instance

To configure this setup, we must decouple the "how" from the "what" using two specific Kubernetes resources.

=== The ServingRuntime (The Template)

The `ServingRuntime` tells KServe *how* to run the container. It defines the Docker image and the startup command.

[source,yaml]
----
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: vllm-runtime
spec:
  containers:
  - name: kserve-container
    image: vllm/vllm-openai:latest
    # The command explicitly starts the Python interpreter to run the OpenAI-compatible server module
    command: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
    args:
      - "--model"
      - "/mnt/models"
      - "--port"
      - "8080"
----

A critical detail in the configuration above is the model path: `/mnt/models`. We point the model path here because KServe automatically mounts downloaded model files to this location.

=== The InferenceService (The Instance)

The `InferenceService` tells KServe *which* specific model to deploy using the template above.

[source,yaml]
----
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-32-prod
spec:
  predictor:
    model:
      modelFormat:
        name: vLLM
      # Point to the specific version folder (e.g., /v1.0) so the Storage Initializer
      # doesn't download unnecessary files from the parent bucket.
      storageUri: "s3://my-bucket/llama2/v1.0"
      runtime: vllm-runtime
      # We must explicitly request GPUs. Setting limit and request to "1"
      # tells Kubernetes to find a node with exactly one available GPU.
      resources:
        limits:
          nvidia.com/gpu: "1"
        requests:
          nvidia.com/gpu: "1"
----

== 4. Deployment Modes: Serverless vs. RawDeployment

KServe offers two primary deployment modes. While Serverless is the default, **RawDeployment** is often preferred for enterprise LLM workloads (like OpenShift AI).

[cols="1,1,1"]
|===
|Feature |Serverless (Knative) |RawDeployment

|**Architecture**
|Request -> Activator -> Queue -> Container
|Request -> Service -> Container

|**Scaling**
|Can scale to 0. Scales on "requests per second".
|Minimum 1. Scales on CPU/RAM metrics (HPA).

|**Latency**
|High "Cold Start" (10-30s).
|Consistent low latency (Always on).

|**Best For**
|Dev/Test, cost savings.
|**Production LLMs**, strict SLAs.
|===

=== Why OpenShift AI uses RawDeployment

In production scenarios, `RawDeployment` is the workhorse for three reasons:

1.  **Avoid Cold Starts:** Booting a large model takes 30-60 seconds. In a chat app, waiting this long is unacceptable.
2.  **GPU Scarcity:** If you scale to zero, you release your GPU. In a busy cluster, another team might grab it before you scale back up.
3.  **Protocol Stability:** Removing the complex Knative proxies improves the stability of long-running streaming responses (token-by-token generation).

== 5. Day 2 Operations: Metrics & Observability

Running an LLM without metrics is dangerous. For vLLM, metrics are critical for monitoring **GPU Memory Management**.

=== Enabling Metrics

In `RawDeployment`, we add annotations to the `InferenceService` to tell Prometheus where to scrape data.

[source,yaml]
----
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-32-prod
spec:
  predictor:
    annotations:
      prometheus.io/scrape: 'true'
      prometheus.io/port: '8080'
      prometheus.io/path: '/metrics'
----

=== The "Big Three" Metrics

Focus your dashboards on these three critical signals:

* **The Fuel Gauge (`vllm:gpu_cache_usage_perc`)**: Measures KV Cache fullness. If this hits >95%, vLLM will pause requests to swap memory.
* **The Speedometer (`vllm:time_per_output_token_seconds`)**: Measures user-perceived latency. Target <50ms per token for an "instant" feel.
* **The Traffic (`vllm:num_requests_waiting`)**: If this is >0, users are stuck in the queue, and your Autoscaler should have already kicked in.