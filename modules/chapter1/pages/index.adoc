= From Registry to Production: Intelligent Model Deployment
:navtitle: Introduction & Value
:toc: macro

// Antora metadata
:page-role: product-concept
:description: Understanding the business value of intelligent model deployment with llm-d in the AI Factory.

[.lead]
*Deploy models intelligently. Maximize GPU ROI.*

You have models ready to deploy—whether from your private Model Registry, the public catalog, or Hugging Face. But deploying a model is only the first step. Making it work *efficiently* at scale is where the real challenge begins.

Standard model deployments treat every request the same. They waste GPU cycles recomputing context that already exists. They route requests randomly, leaving some pods overloaded while others sit idle. The result? You pay for expensive infrastructure that delivers suboptimal performance.

This course teaches you to deploy models using **Distributed Inference with `llm-d`**—the intelligent routing framework that transforms expensive, idle infrastructure into a cost-efficient, high-performance inference service.

[NOTE]
.The Core Business Problem
====
*"Why should we deploy with `llm-d` when standard vLLM deployments work fine?"*

**The Answer:** Standard deployments treat every request the same. They waste GPU cycles recomputing context that already exists. `llm-d` provides **intelligent routing** and **KV cache off-loading** that ensures every model deployment—whether from your registry or the public catalog—operates at maximum efficiency.
====

== The Challenge: The "Idle GPU Tax"

When you deploy a model using standard methods, you face a hidden cost problem:

* **The Cold Start Penalty:** Every new conversation requires the model to process the entire prompt from scratch (the "Prefill" phase). This is computationally expensive and slow.
* **The Cache Waste:** When a user returns to continue a conversation, standard deployments cannot "remember" where that context lives. The system recomputes everything, burning GPU cycles and increasing latency.
* **The Resource Inefficiency:** Without intelligent routing, requests are distributed randomly. One pod might be overloaded while another sits idle, wasting expensive hardware.

The result? You pay for GPU capacity that delivers suboptimal performance.

== The Solution: Intelligent Routing with llm-d

**Distributed Inference with `llm-d`** solves this by adding a "Traffic Control" layer above your model deployments. It provides two critical capabilities that apply to **all model deployments**, regardless of source:

=== 1. Intelligent, Cache-Aware Routing

Unlike standard round-robin load balancing, `llm-d` maintains a real-time map of which pods hold which conversation contexts (KV Cache). When a request arrives:

* **Cache Hit (Fast Path):** The scheduler routes the request directly to the pod that already has the context loaded. This eliminates expensive Prefill computation and delivers near-instant response times.
* **Cache Miss (Slow Path):** The scheduler routes to an available Prefill pod, processes the prompt, then routes subsequent tokens to a Decode pod for generation.

**The Win:** You maximize GPU utilization by avoiding redundant computation.

**The Benefit:** Users experience consistent, low-latency responses even during peak load.

=== 2. KV Cache Off-Loading for All Deployments

The KV Cache (Key-Value Cache) stores the conversation context in GPU memory. `llm-d` manages this cache intelligently across your entire fleet:

* **Cache Affinity:** Requests are routed to pods that already hold the relevant cache, reducing memory pressure and compute costs.
* **Cache Sharing:** In advanced configurations, the cache can be shared across pods, enabling even greater efficiency for high-concurrency scenarios.

**The Win:** Every model deployment—whether from your private registry or the public catalog—benefits from intelligent cache management.

**The Benefit:** You can serve more concurrent users on the same hardware, directly improving your cost-per-token ratio.

== The Complete Deployment Story

This course covers the full lifecycle of intelligent model deployment:

1. **The Foundation:** Understand how `llm-d` provides intelligent routing and KV cache management for all model deployments.
2. **The Deployment:** Deploy your model using `llm-d`, whether from your private registry, the public catalog, or Hugging Face.
3. **The Payoff:** Your model serves users efficiently, with every GPU cycle optimized for performance and cost.

[IMPORTANT]
.Prerequisites
====
To successfully complete the hands-on sections of this course, you need:

* Access to a **Red Hat OpenShift AI 3.0** cluster.
* A model ready to deploy (from your private registry, the public catalog, or Hugging Face). We will use `Qwen/Qwen3-0.6B` as an example.
* `cluster-admin` privileges (to install required operators and configure the Gateway API).
* The `oc` CLI tool installed in your terminal.
====

== Your Mission: Deploy with Intelligence

In this course, you will not just deploy a model; you will deploy it **intelligently**. You will take on the role of a Platform Engineer tasked with transforming a registered model into a production-grade inference service.

**You will execute the following Technical Workflow:**

 1.  **The Architecture:** Understand how `llm-d` provides intelligent routing and KV cache off-loading for all model deployments.
 2.  **The Foundation:** Verify Gateway API support and install operators (if needed for multi-node or MoE deployments).
 3.  **The Deployment:** Deploy your model using the `llm-d` runtime, configuring intelligent routing and cache management.
 4.  **The Validation:** Verify that requests are routed intelligently and that KV cache hits reduce latency and cost.

---
*Ready to deploy? Let's start by understanding the architecture.*
