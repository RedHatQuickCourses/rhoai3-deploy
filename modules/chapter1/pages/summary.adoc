= Course Summary: The Engineering Reality
:navtitle: Summary & Next Steps
:toc: macro

// Antora metadata
:page-role: Graduate
:description: Recap of the Enterprise Serving workflow and next steps for production.

[.lead]
*You have graduated from "Shadow AI."*

At the start of this course, we identified the core problem:
*Downloading models to a laptop is easy. Serving them at scale, on budget, and without crashing production is hard.*

You have now built a governed **AI Serving Factory**. Let's review the engineering decisions you made to get here.

== The Journey Recap

[cols="1,3"]
|===
| Phase | The Engineering Decision

| **1. Strategy**
| Instead of grabbing random models, you selected **Granite-3.3-2B** from the **Red Hat AI Validated Repository**.
*Result:* You gained legal certainty (Apache 2.0) and performance predictability.

| **2. Architecture**
| Instead of guessing, you matched the hardware to the workload.
*Result:* You avoided deploying an FP16 model on incompatible hardware or over-provisioning an H100 for a tiny task.

| **3. Sizing**
| You performed the VRAM math: `(Weights * 1.2) + KV Cache`.
*Result:* You identified that the default 128k context window would crash your A10G GPU.

| **4. Tuning**
| You configured **vLLM** with `--max-model-len=8192`.
*Result:* You created a "Safety Valve" that prevents OOM errors under high load.

| **5. Operations**
| You abandoned the UI for a GitOps script (`serve_model.sh`) from the `rhoai3-deploy` repository.
*Result:* Your deployment is now reproducible, version-controlled, and ready for automation.
|===

== Next Steps: Day 2 Operations

Serving the model is just Day 1. To run a true production platform, your next steps are:

* **1. Observability:**
Configure **Prometheus** to scrape the vLLM metrics port (`8080`).
* *Key Metric:* `vllm:num_requests_running` (Concurrency) and `vllm:gpu_cache_usage_perc` (Are we running out of KV Cache?).

* **2. Autoscaling:**
Configure **KNative** to scale your pods based on concurrency.
* *Trigger:* If `concurrency > 10`, spin up a second GPU pod.

* **3. Quantization:**
Explore the **Quantization Labs** (coming soon) to shrink your Granite-2B model to **INT8**.
* *Goal:* Double your throughput on the same hardware.

---
*Congratulations. You are now ready to serve AI responsibly.*