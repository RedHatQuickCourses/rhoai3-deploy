= The Engine: vLLM & PagedAttention
:navtitle: vLLM Concepts
:toc: macro

// Antora metadata
:page-role: MLOps Architect
:description: Understanding PagedAttention and why vLLM is the standard for OpenShift AI.

[.lead]
*Inference is a memory management problem disguised as a compute problem.*

You have selected your model. You have sized your hardware. Now you need software to bridge the gap.
In the early days of LLMs, we used naive loaders that allocated contiguous blocks of memory for every user request.
This resulted in massive fragmentation. A 24GB GPU would report "Out of Memory" while actually having 10GB of fragmented, unusable free space.

Enter **vLLM**, the standard inference engine for Red Hat OpenShift AI.

== The Core Innovation: PagedAttention

vLLM changed the industry by treating GPU memory the way an operating system treats RAM: via **Paging**.

* **The old way:** If a user *might* generate 2,000 tokens, the system reserved a continuous 2,000-token block of VRAM immediately. If they only generated 50 tokens, the rest was wasted.
* **The vLLM way (PagedAttention):** It breaks the Key-Value (KV) cache into small, non-contiguous blocks ("pages"). It allocates these pages on-demand.



=== The Engineering Impact

1.  **Zero Waste:** vLLM can fill strictly available non-contiguous memory slots.
2.  **Higher Throughput:** Because memory isn't wasted on "reservation," you can fit 2x-4x more concurrent users (Batch Size) on the same hardware.
3.  **The Trade-off:** vLLM is optimized for **Throughput** (serving many users) rather than pure **Latency** (serving one user instantly). For enterprise serving, throughput is usually the priority.

== The Interface: OpenAI Compatibility

vLLM provides a drop-in replacement for the OpenAI API.
This is a strategic advantage for platform engineers:

* **No Vendor Lock-in:** Your developers can write code using the standard `openai` Python library.
* **Migration Path:** You can prototype on public APIs (GPT-4) and switch to your private Granite deployment simply by changing the `base_url` environment variable.

[source,python]
----
# The code doesn't change. Only the endpoint does.
client = OpenAI(
    base_url="http://granite-service.rhoai.svc:8080/v1",
    api_key="empty" # vLLM doesn't require keys by default
)
----

xref:vllm-tuning.adoc[Next: Tuning the Configuration]