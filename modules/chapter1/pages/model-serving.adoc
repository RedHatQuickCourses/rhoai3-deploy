= Lab: Automated Deployment (GitOps)
:navtitle: Automated Deployment
:toc: macro

// Antora metadata
:page-role: Platform Engineer
:description: Replacing manual UI clicks with a reproducible infrastructure script.

[.lead]
*Click-Ops does not scale. GitOps does.*

In the previous modules, we selected the **granite-3.3-2b-instruct** model and calculated the tuning parameters (`max-model-len=16000`) required to make it run safely on our hardware.
Now, we must deploy it.

You *could* open the OpenShift AI Dashboard, click "Add Model Server," copy-paste arguments, and hope you didn't make a typo.

But what happens when you need to deploy 50 models? Or when you need to update the configuration across three clusters?

In this lab, We will deploy our tuned inference stack using a parameterized **Infrastructure-as-Code** script.


== Fast Track to Model Deployment Setup

To begin, we need to acquire the automation scripts. We have packaged the entire "inference deployment" into a Git repository.

. **Open your OpenShift Terminal.**
. **Clone the repository:**
+
[source,bash]
----
git clone https://github.com/RedHatQuickCourses/rhoai3-deploy.git
----
+
**Switch to the lab directory:**
+
[source,bash]
----
cd rhoai3-deploy
----

---

[IMPORTANT]
====
This utility automates the "grunt work" of the AI Engineer:

 1.  Deploys a MinIO Object Store ("The Vault").
 
 2.  Creates a Data Connection in OpenShift AI.

 3.  Downloads a model (`granite-3.3-2b-instruct`) and places it in the vault (S3 storage).

 4. Create a 'fast-track-sa' service account to access the storage. 

This script only works with **Public/Ungated** models (like IBM Granite 4.0 or Qwen 2.5). 
It does not support gated models (like Llama 3) that require a Hugging Face Token.
====

== Running the Script

1.  Open your OpenShift Terminal.
2.  Navigate to the repository root.
3.  Execute the script:
+
[source,bash]
----
chmod u+x deploy/fast-track.sh
./deploy/fast-track.sh
----

=== Verification

Once the script completes (look for `âœ… SUCCESS`), verify the environment is ready for serving:



== Model Inference 

Now, we must deploy the **Granite 3.3 2B** model using a reproducible KServe manifest.

You *could* open the OpenShift AI Dashboard, click "Add Model Server," copy-paste arguments, and hope you didn't make a typo. But this approach fails when you need to deploy 50 models or update configurations across multiple clusters.

In this lab, we will deploy our inference stack using **Infrastructure-as-Code**.


=== Step 1: Prerequisites

Before proceeding, verify that your "Fast Track" script successfully populated the vault.

=== **Check the S3 Bucket:**

[source,bash]
----
# Verify the model files exist in the 'models' bucket
oc exec -n model-deploy-lab deployment/minio -- ls /data/models/granite3
----

*Expected Output:* You should see files like `config.json`, `tokenizer.json`, and `*.safetensors`.


=== **Check the Data Connection:**

[source,bash]
----
# Verify the KServe storage secret exists
oc get secret storage-config -n model-deploy-lab
----

== Step 2: The Runtime (The Engine)

RHOAI includes a certified vLLM runtime template. Instead of writing one from scratch, we will extract the official template from the cluster and install it in our namespace.

=== **Extract and Apply the Runtime:**

[source,bash]
----
oc process vllm-cuda-runtime-template -n redhat-ods-applications | \
oc apply -f - -n model-deploy-lab
----

[NOTE]
====
To view the runtime config, leave off the:
** | \ oc apply -f - -n model-deploy-lab**, from the previous command. 
====

**Verify the Runtime:**

[source,bash]
----
oc get servingruntime vllm-cuda-runtime -n model-deploy-lab
----

== Create an Secret to Access the Models Files

[source,bash]
.data-connection.yaml
----
apiVersion: v1
kind: Secret
metadata:
  name: models-connection  # Renaming for clarity (was 'models' or 'storage-config')
  namespace: model-deploy-lab
  labels:
    # 1. VISIBILITY: Tells the Dashboard "I belong to RHOAI"
    opendatahub.io/dashboard: "true"
    # 2. MANAGEMENT: Tells RHOAI "I am a managed Data Connection"
    opendatahub.io/managed: "true"
  annotations:
    # 3. TYPE: Tells the Injector "I am an S3 credential"
    opendatahub.io/connection-type: s3
    openshift.io/display-name: Models Bucket
type: Opaque
stringData:
  # The keys MUST match standard AWS SDK expectations
  AWS_ACCESS_KEY_ID: minio
  AWS_SECRET_ACCESS_KEY: minio123
  AWS_DEFAULT_REGION: us-east-1
  AWS_S3_ENDPOINT: http://minio-service.model-deploy-lab:9000
  AWS_S3_BUCKET: models

----

[source,bash]
.link the accounts.yaml
----
# 1. Apply the corrected secret
oc apply -f data-connection.yaml

# 2. Link it to the Service Account your ISVC is using
# (Assuming your ISVC uses 'fast-track-sa' based on previous steps)
oc secret link fast-track-sa models-connection

----

== Step 3: The Deployment (The Workload)

We will now create the `InferenceService`. This Custom Resource binds our **Runtime** (the engine) to our **Storage** (the fuel).

=== **Create the Manifest:**
Copy the following YAML to a file named `granite3-isvc.yaml`.

[source,yaml]
----
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: granite3
  namespace: model-deploy-lab
  annotations:
    # "RawDeployment" mode bypasses Serverless to prevent cold starts
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    serviceAccountName: fast-track-sa
    model:
      # The Format must match what the Runtime supports (vLLM)
      modelFormat:
        name: vLLM
      # This name must match the runtime extracted in Step 2
      runtime: vllm-cuda-runtime
      # The path to your model in MinIO (s3://bucket/folder)
      storageUri: s3://models/granite3
      resources:
        requests:
          nvidia.com/gpu: "1"
          memory: "8Gi"
        limits:
          nvidia.com/gpu: "1"
          memory: "10Gi"
----

=== **Apply the Deployment:**

[source,bash]
----
oc apply -f granite3-isvc.yaml
----

=== **Wait for Readiness:**

This process can take 2-5 minutes as the vLLM engine initializes and loads the weights into GPU memory.

[source,bash]
----
oc wait --for=condition=Ready inferenceservice/granite3 -n model-deploy-lab --timeout=300s
----

== Step 4: Validate the API

Once the model is ready, we can test it using the OpenAI-compatible API provided by vLLM.

=== **Retrieve the Inference URL:**

[source,bash]
----
export MODEL_URL=$(oc get inferenceservice granite3 -n model-deploy-lab -o jsonpath='{.status.url}')
echo "Targeting: $MODEL_URL"
----

// # Add the port explicitly
[source,bash]
----
curl -k -X POST "${MODEL_URL}:8080/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "granite3",
    "messages": [{"role": "user", "content": "Write a Poem about AI"}]
  }'
----

*Expected Output:*

[source,bash]
----

{
  "id": "cmpl-...",
  "object": "chat.completion",
  "created": 1700000000,
  "model": "granite3",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Pods rise in the cloud,\nGPUs hum with quiet power,\nCode serves thought to all."
      },
      "finish_reason": "stop"
    }
  ]
}
----