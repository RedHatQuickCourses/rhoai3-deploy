= Lab: Automated Deployment (GitOps)
:navtitle: Automated Deployment
:toc: macro

// Antora metadata
:page-role: Platform Engineer
:description: Replacing manual UI clicks with a reproducible infrastructure script.

[.lead]
*Click-Ops does not scale. GitOps does.*

In the previous modules, we selected the *Granite-8B (FP16)* model and calculated the tuning parameters (`max-model-len=8000`) required to make it run safely on our hardware.

Now, we're using the **granite-3.3-2b-instruct**, it's .75% smaller and faster experience. 


You *could* open the OpenShift AI Dashboard, click "Add Model Server," copy-paste arguments, and hope you didn't make a typo.

But what happens when you need to deploy 50 models? Or when you need to update the configuration across three clusters?

In this lab, We will deploy our tuned inference stack using a parameterized **Infrastructure-as-Code** scripts.


== Fast Track to Model Deployment Setup

To begin, we need to acquire the automation scripts. We have packaged the entire "inference deployment" into a Git repository.

. **Open your OpenShift Terminal.**
. **Clone the repository:**
+
[source,bash]
----
git clone https://github.com/RedHatQuickCourses/rhoai3-deploy.git
----
+
**Switch to the lab directory:**
+
[source,bash]
----
cd rhoai3-deploy
----

---

[IMPORTANT]
====
This utility automates the "grunt work" of the AI Engineer:

 1.  Deploys a MinIO Object Store ("The Vault").
 
 2.  Creates a Data Connection in OpenShift AI.

 3.  Downloads a model (`granite-3.3-2b-instruct`) and places it in the vault (S3 storage).

 4. Create a 'fast-track-sa' service account to access the storage. 

This script only works with **Public/Ungated** models (like IBM Granite 4.0 or Qwen 2.5). 
It does not support gated models (like Llama 3) that require a Hugging Face Token.
====

== Running the Script

1.  Open your OpenShift Terminal.
2.  Navigate to the repository root.
3.  Execute the script:
+
[source,bash]
----
chmod u+x deploy/fast-track.sh
./deploy/fast-track.sh
----

=== Verification

Once the script completes (look for `âœ… SUCCESS`), verify the environment is ready for serving:



== Model Inference 


=== Step 1: Prerequisites

Before proceeding, verify that your "Fast Track" script successfully populated the vault.

=== **Check the S3 Bucket:**

[source,bash]
----
# Verify the model files exist in the 'models' bucket
oc exec -n model-deploy-lab deployment/minio -- ls /data/models/granite3
----

*Expected Output:* You should see files like `config.json`, `tokenizer.json`, and `*.safetensors`.


=== **Check the Data Connection:**

[source,bash]
----
# Verify the KServe storage secret exists
oc get secret storage-config -n model-deploy-lab
----

== Step 2: The Runtime (The Engine)

RHOAI includes a certified vLLM runtime template. Instead of writing one from scratch, we will extract the official template from the cluster and install it in our namespace.

=== **Extract and Apply the Runtime:**

[source,bash]
----
oc process vllm-cuda-runtime-template -n redhat-ods-applications | \
oc apply -f - -n model-deploy-lab
----

[NOTE]
====
To view the runtime config, leave off the:
** | \ oc apply -f - -n model-deploy-lab**, from the previous command. 
====

**Verify the Runtime:**

[source,bash]
----
oc get servingruntime vllm-cuda-runtime -n model-deploy-lab
----

== Create a Secret to Access the Models Files

In order to read the model files from the "vault" storage, we need a storage-config, which is stored as a kubernets secret.
There is supplied file in the deploy folder called dataconn.yaml.  Apply this file to create the secret.


[source,bash]
.dataconn.yaml
----
apiVersion: v1
kind: Secret
metadata:
  name: models-connection  # Renaming for clarity (was 'models' or 'storage-config')
  namespace: model-deploy-lab
  labels:
    # 1. VISIBILITY: Tells the Dashboard "I belong to RHOAI"
    opendatahub.io/dashboard: "true"
    # 2. MANAGEMENT: Tells RHOAI "I am a managed Data Connection"
    opendatahub.io/managed: "true"
  annotations:
    # 3. TYPE: Tells the Injector "I am an S3 credential"
    opendatahub.io/connection-type: s3
    openshift.io/display-name: Models Bucket
type: Opaque
stringData:
  # The keys MUST match standard AWS SDK expectations
  AWS_ACCESS_KEY_ID: minio
  AWS_SECRET_ACCESS_KEY: minio123
  AWS_DEFAULT_REGION: us-east-1
  AWS_S3_ENDPOINT: http://minio-service.model-deploy-lab:9000
  AWS_S3_BUCKET: models

----

[source,bash]
.deploy the storage secret for use by the deployment
----
oc apply -f ./deploy/dataconn.yaml
----

[source,bash]
.links data-connection to fast-track service account
----
oc secret link fast-track-sa models-connection
----

== Step 3: The Deployment (The Workload)

We will now create the `InferenceService`. This Custom Resource binds our **Runtime** (the engine) to our **Storage** (the fuel).

=== **Create the Manifest:**
Copy the following YAML to a file named `granite3-isvc.yaml`.

[source,yaml]
----
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: granite3
  namespace: model-deploy-lab
  annotations:
    # "RawDeployment" mode bypasses Serverless to prevent cold starts
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    serviceAccountName: fast-track-sa
    model:
      # The Format must match what the Runtime supports (vLLM)
      modelFormat:
        name: vLLM
      # This name must match the runtime extracted in Step 2
      runtime: vllm-cuda-runtime
      # The path to your model in MinIO (s3://bucket/folder)
      storageUri: s3://models/granite3
      args:
        - "--max-model-len=4096"        # Limits context to 4k to save VRAM
        - "--gpu-memory-utilization=0.95" # Uses 95% of GPU (leaves 5% for overhead)
       # - "--distributed-executor-backend=mp" # Required for some multi-GPU setups
      resources:
        requests:
          nvidia.com/gpu: "1"
          memory: "8Gi"
        limits:
          nvidia.com/gpu: "1"
          memory: "10Gi"
----

=== **Apply the Deployment:**

[source,bash]
----
oc apply -f ./deploy/deploy.yaml
----

=== **Wait for Readiness:**

This process can take 2-5 minutes as the vLLM engine initializes and loads the weights into GPU memory.

[source,bash]
----
oc wait --for=condition=Ready inferenceservice/granite3 -n model-deploy-lab --timeout=300s
----

== Step 4: Validate the API

Once the model is ready, we can test it using the OpenAI-compatible API provided by vLLM.

=== **Retrieve the Inference URL:**

[source,bash]
----
export MODEL_URL=$(oc get inferenceservice granite3 -n model-deploy-lab -o jsonpath='{.status.url}')
echo "Targeting: $MODEL_URL"
----

// # Add the port explicitly
[source,bash]
----
curl -k -X POST "${MODEL_URL}:8080/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "granite3",
    "messages": [{"role": "user", "content": "Write a Poem about AI"}]
  }'
----

*Expected Output:*

[source,bash]
----

{
  "id": "cmpl-...",
  "object": "chat.completion",
  "created": 1700000000,
  "model": "granite3",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Pods rise in the cloud,\nGPUs hum with quiet power,\nCode serves thought to all."
      },
      "finish_reason": "stop"
    }
  ]
}
----

---

=== That's a Wrap

This is one example of model inference using the built-in runtime from the command line. 

Based on your feedback, we'll update this course accordingly: 

 * What type of model inference examples should be add to this course?

 * What additional features need to be supported such as tool calling templates?


Let us know by https://github.com/RedHatQuickCourses/rhoai3-deploy/issues[creating an issue in the course repository, window=blank] 

