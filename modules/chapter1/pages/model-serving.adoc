= Lab: Automated Deployment (GitOps)
:navtitle: Automated Deployment
:toc: macro

// Antora metadata
:page-role: Platform Engineer
:description: Replacing manual UI clicks with a reproducible infrastructure script.

[.lead]
*Click-Ops does not scale. GitOps does.*

To begin, we need to acquire the automation scripts. We have packaged the entire "Serving Factory" into a Git repository.

. **Open your OpenShift Terminal.**
. **Clone the repository:**
+
[source,bash]
----
git clone https://github.com/RedHatQuickCourses/rhoai3-deploy.git
----

**Switch to the lab directory:**

[source,bash]
----
cd rhoai3-deploy
----

[IMPORTANT]
.Are you catching up?
====
If you skipped the previous labs, your environment might be empty (no MinIO, no Model weights).
Before proceeding, you must ensure the model is downloaded and the storage is configured.
**xref:fast-track.adoc[Click here to run the Fast Track Setup Script]** to get your environment ready in 2 minutes.
====

. Now, we must deploy the **Granite 3.3 2B** model using a reproducible KServe manifest.

You *could* open the OpenShift AI Dashboard, click "Add Model Server," copy-paste arguments, and hope you didn't make a typo. But this approach fails when you need to deploy 50 models or update configurations across multiple clusters.

In this lab, we will deploy our inference stack using **Infrastructure-as-Code**.


== Step 1: Prerequisites

Before proceeding, verify that your "Fast Track" script successfully populated the vault.

=== **Check the S3 Bucket:**

[source,bash]
----
# Verify the model files exist in the 'models' bucket
oc exec -n model-deploy-lab deployment/minio -- ls /data/models/granite3
----

*Expected Output:* You should see files like `config.json`, `tokenizer.json`, and `*.safetensors`.


=== **Check the Data Connection:**

[source,bash]
----
# Verify the KServe storage secret exists
oc get secret storage-config -n model-deploy-lab
----

== Step 2: The Runtime (The Engine)

RHOAI includes a certified vLLM runtime template. Instead of writing one from scratch, we will extract the official template from the cluster and install it in our namespace.

=== **Extract and Apply the Runtime:**

[source,bash]
----
oc process vllm-cuda-runtime-template -n redhat-ods-applications | \
oc apply -f - -n model-deploy-lab
----

=== **Verify the Runtime:**

[source,bash]
----
oc get servingruntime vllm-cuda-runtime -n model-deploy-lab
----

[source,bash]
.data-connection.yaml
----
apiVersion: v1
kind: Secret
metadata:
  name: models-connection  # Renaming for clarity (was 'models' or 'storage-config')
  namespace: model-deploy-lab
  labels:
    # 1. VISIBILITY: Tells the Dashboard "I belong to RHOAI"
    opendatahub.io/dashboard: "true"
    # 2. MANAGEMENT: Tells RHOAI "I am a managed Data Connection"
    opendatahub.io/managed: "true"
  annotations:
    # 3. TYPE: Tells the Injector "I am an S3 credential"
    opendatahub.io/connection-type: s3
    openshift.io/display-name: Models Bucket
type: Opaque
stringData:
  # The keys MUST match standard AWS SDK expectations
  AWS_ACCESS_KEY_ID: minio
  AWS_SECRET_ACCESS_KEY: minio123
  AWS_DEFAULT_REGION: us-east-1
  AWS_S3_ENDPOINT: http://minio-service.model-deploy-lab:9000
  AWS_S3_BUCKET: models

----

[source,bash]
.link the accounts.yaml
----
# 1. Apply the corrected secret
oc apply -f data-connection.yaml

# 2. Link it to the Service Account your ISVC is using
# (Assuming your ISVC uses 'fast-track-sa' based on previous steps)
oc secret link fast-track-sa models-connection

# 3. Trigger a restart of the pod
oc delete pod -l serving.kserve.io/inferenceservice=granite3

----

== Step 3: The Deployment (The Workload)

We will now create the `InferenceService`. This Custom Resource binds our **Runtime** (the engine) to our **Storage** (the fuel).

=== **Create the Manifest:**
Copy the following YAML to a file named `granite3-isvc.yaml`.

[source,yaml]
----
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: granite3
  namespace: model-deploy-lab
  annotations:
    # "RawDeployment" mode bypasses Serverless to prevent cold starts
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    serviceAccountName: fast-track-sa
    model:
      # The Format must match what the Runtime supports (vLLM)
      modelFormat:
        name: vLLM
      # This name must match the runtime extracted in Step 2
      runtime: vllm-cuda-runtime
      # The path to your model in MinIO (s3://bucket/folder)
      storageUri: s3://models/granite3
      resources:
        requests:
          nvidia.com/gpu: "1"
          memory: "8Gi"
        limits:
          nvidia.com/gpu: "1"
          memory: "10Gi"
----

=== **Apply the Deployment:**

[source,bash]
----
oc apply -f granite3-isvc.yaml
----

=== **Wait for Readiness:**

This process can take 2-5 minutes as the vLLM engine initializes and loads the weights into GPU memory.

[source,bash]
----
oc wait --for=condition=Ready inferenceservice/granite3 -n model-deploy-lab --timeout=300s
----

== Step 4: Validate the API

Once the model is ready, we can test it using the OpenAI-compatible API provided by vLLM.

=== **Retrieve the Inference URL:**

[source,bash]
----
export MODEL_URL=$(oc get inferenceservice granite3 -n model-deploy-lab -o jsonpath='{.status.url}')
echo "Targeting: $MODEL_URL"
----

// # Add the port explicitly
[source,bash]
----
curl -k -X POST "${MODEL_URL}:8080/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "granite3",
    "messages": [{"role": "user", "content": "Write a Poem about AI"}]
  }'
----

*Expected Output:*

{
  "id": "cmpl-...",
  "object": "chat.completion",
  "created": 1700000000,
  "model": "granite3",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Pods rise in the cloud,\nGPUs hum with quiet power,\nCode serves thought to all."
      },
      "finish_reason": "stop"
    }
  ]
}
