= Enterprise Model Serving at Scale
:navtitle: Strategy & Selection
:toc: macro

// Antora metadata
:page-role: MLOps Engineer
:description: Moving from "Shadow AI" downloads to fast, cost-effective, and governed model serving.

[.lead]
*Inference is where the bill arrives.*

In the experimentation phase, the only metric that matters is "intelligence"—how smart is the model?
But in the production phase, the metrics change. Suddenly, you are judged on **Latency**, **Throughput**, and **Cost Per Token**.

A developer might download a 70-billion parameter model to a laptop and be impressed by its reasoning.
A Platform Engineer looks at that same model and sees a massive infrastructure bill, potential `OutofMemory` errors, and a latency bottleneck that could kill the user experience.

We are moving beyond "Shadow AI"—where users blindly deploy heavy models—to a governed "AI Factory" where inference is fast, efficient, and cost-effective by design.

== The Core Challenge: "Will It Fit?"

Deploying an LLM is not like deploying a microservice. You cannot just throw it into a container and hope for the best.
You must answer three critical questions before you ever run a `kubectl apply`:

 1.  **The Selection:** Is this model the right size for the task, or are we burning GPU cycles on overkill?
 2.  **The Hardware:** Do we have enough Video RAM (VRAM) to handle the model weights *plus* the concurrent user traffic (KV Cache)?
 3.  **The Engine:** Is our runtime (vLLM) tuned to maximize throughput, or are we leaving performance on the table?

== The Serving Workflow

In this course, we will break down the black box of model serving into a repeatable engineering process.

=== 1. Smart Model Selection
You don't have to benchmark 900,000 models from scratch.
We will explore tools like the **Red Hat AI Validated Model Repository**, which acts as a shortcut. It provides pre-vetted models with known performance profiles, allowing you to skip the "guesswork" phase and select models that are guaranteed to work on your hardware.

=== 2. Hardware Sizing (The Math)
We will teach you the formula to calculate the true cost of a model.

 * **Weights vs. Cache:** Understanding why a model that "fits" when idle crashes when under load.
 * **Quantization:** How to use formats like `FP8` or `INT8` to run massive models on smaller, cheaper GPUs without sacrificing intelligence.

=== 3. The Engine: vLLM Tuning
We will look under the hood of **vLLM**, the standard inference engine for OpenShift AI.
You will learn to tune critical parameters like:

 * `max-model-len`: Managing the context window to prevent memory overflows.
 * `gpu-memory-utilization`: Optimizing VRAM allocation for high-throughput batching.

=== 4. Automated Deployment (GitOps)
Finally, we will abandon the "Click-Ops" dashboard.
You will deploy a production-ready **InferenceService** using a parameterized script. This moves your infrastructure from "manual configuration" to "code," enabling reproducible and scalable deployments.

== Our Laboratory Model: granite-3.3b-instruct

To practice these concepts without requiring a massive GPU cluster, we will standardize on the **IBM granite-3.3b-instruct** model.

* **Why this model?**

    * **Efficiency:** Its small size (3 Billion parameters) allows us to deploy it quickly and experiment with tuning parameters on standard hardware.
    * **Modern Architecture:** It uses the same underlying technology as massive 70B+ models, meaning the sizing and tuning lessons you learn here directly apply to enterprise-scale deployments.
    * **Transparency:** It is fully open-source (Apache 2.0) with disclosed training data, representing the gold standard for trusted enterprise AI.

[IMPORTANT]
.Prerequisites
====
To successfully complete the hands-on sections of this course, you need:

 * Access to a **Red Hat OpenShift AI** cluster.
 * A basic understanding of Kubernetes (Pods, Services, Routes).
 * The `oc` CLI tool installed in your terminal.
====

---
*The goal is efficient inference. Let's start by calculating the cost.*


oc get pod -l serving.kserve.io/inferenceservice=granite3 \
  -n model-deploy-lab \
  -o jsonpath='{.items[0].spec.containers[0].args}'

  export URL=$(oc get inferenceservice granite3 -n model-deploy-lab -o jsonpath='{.status.url}')

curl -k $URL/v1 \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "granite3",
    "prompt": "Write a haiku about GitOps.",
    "max_tokens": 60
  }'