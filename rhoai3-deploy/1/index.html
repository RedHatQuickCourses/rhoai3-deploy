<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Deploying Models with Intelligent Routing :: Intelligent Model Deployment with llm-d</title>
    <link rel="next" href="chapter1/section1.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../..">Intelligent Model Deployment with llm-d</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai3-deploy" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="index.html">Intelligent Model Deployment with llm-d</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="index.html">Introduction &amp; Value</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter1/section1.html">Architecture Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter1/section2.html">The Deployment Lab</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter1/section3.html">Troubleshooting</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter1/index.html">Introduction &amp; Value</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter1/section1.html">Architecture Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter1/section2.html">The Deployment Lab</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="chapter1/section3.html">Troubleshooting</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Intelligent Model Deployment with llm-d</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="index.html">Intelligent Model Deployment with llm-d</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="index.html" class="home-link is-current"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="index.html">Intelligent Model Deployment with llm-d</a></li>
    <li><a href="index.html">Introduction &amp; Value</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Deploying Models with Intelligent Routing</h1>
<h1 id="_from_registry_to_production_intelligent_model_deployment" class="sect0"><a class="anchor" href="#_from_registry_to_production_intelligent_model_deployment"></a>From Registry to Production: Intelligent Model Deployment</h1>
<div class="paragraph lead">
<p><strong>Deploy models intelligently. Maximize GPU ROI.</strong></p>
</div>
<div class="paragraph">
<p>You have models ready to deploy—whether from your private Model Registry, the public catalog, or Hugging Face. But deploying a model is only the first step. Making it work <strong>efficiently</strong> at scale is where the real challenge begins.</p>
</div>
<div class="paragraph">
<p>Standard model deployments treat every request the same. They waste GPU cycles recomputing context that already exists. They route requests randomly, leaving some pods overloaded while others sit idle. The result? You pay for expensive infrastructure that delivers suboptimal performance.</p>
</div>
<div class="paragraph">
<p>This course teaches you to deploy models using <strong>Distributed Inference with <code>llm-d</code></strong>—the intelligent routing framework that transforms expensive, idle infrastructure into a cost-efficient, high-performance inference service.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">The Core Business Problem</div>
<div class="paragraph">
<p><strong>"Why should we deploy with <code>llm-d</code> when standard vLLM deployments work fine?"</strong></p>
</div>
<div class="paragraph">
<p><strong>The Answer:</strong> Standard deployments treat every request the same. They waste GPU cycles recomputing context that already exists. <code>llm-d</code> provides <strong>intelligent routing</strong> and <strong>KV cache off-loading</strong> that ensures every model deployment—whether from your registry or the public catalog—operates at maximum efficiency.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect1">
<h2 id="_the_challenge_the_idle_gpu_tax"><a class="anchor" href="#_the_challenge_the_idle_gpu_tax"></a>The Challenge: The "Idle GPU Tax"</h2>
<div class="sectionbody">
<div class="paragraph">
<p>When you deploy a model using standard methods, you face a hidden cost problem:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>The Cold Start Penalty:</strong> Every new conversation requires the model to process the entire prompt from scratch (the "Prefill" phase). This is computationally expensive and slow.</p>
</li>
<li>
<p><strong>The Cache Waste:</strong> When a user returns to continue a conversation, standard deployments cannot "remember" where that context lives. The system recomputes everything, burning GPU cycles and increasing latency.</p>
</li>
<li>
<p><strong>The Resource Inefficiency:</strong> Without intelligent routing, requests are distributed randomly. One pod might be overloaded while another sits idle, wasting expensive hardware.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The result? You pay for GPU capacity that delivers suboptimal performance.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_solution_intelligent_routing_with_llm_d"><a class="anchor" href="#_the_solution_intelligent_routing_with_llm_d"></a>The Solution: Intelligent Routing with llm-d</h2>
<div class="sectionbody">
<div class="paragraph">
<p><strong>Distributed Inference with <code>llm-d</code></strong> solves this by adding a "Traffic Control" layer above your model deployments. It provides two critical capabilities that apply to <strong>all model deployments</strong>, regardless of source:</p>
</div>
<div class="sect2">
<h3 id="_1_intelligent_cache_aware_routing"><a class="anchor" href="#_1_intelligent_cache_aware_routing"></a>1. Intelligent, Cache-Aware Routing</h3>
<div class="paragraph">
<p>Unlike standard round-robin load balancing, <code>llm-d</code> maintains a real-time map of which pods hold which conversation contexts (KV Cache). When a request arrives:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Cache Hit (Fast Path):</strong> The scheduler routes the request directly to the pod that already has the context loaded. This eliminates expensive Prefill computation and delivers near-instant response times.</p>
</li>
<li>
<p><strong>Cache Miss (Slow Path):</strong> The scheduler routes to an available Prefill pod, processes the prompt, then routes subsequent tokens to a Decode pod for generation.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>The Win:</strong> You maximize GPU utilization by avoiding redundant computation.</p>
</div>
<div class="paragraph">
<p><strong>The Benefit:</strong> Users experience consistent, low-latency responses even during peak load.</p>
</div>
</div>
<div class="sect2">
<h3 id="_2_kv_cache_off_loading_for_all_deployments"><a class="anchor" href="#_2_kv_cache_off_loading_for_all_deployments"></a>2. KV Cache Off-Loading for All Deployments</h3>
<div class="paragraph">
<p>The KV Cache (Key-Value Cache) stores the conversation context in GPU memory. <code>llm-d</code> manages this cache intelligently across your entire fleet:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Cache Affinity:</strong> Requests are routed to pods that already hold the relevant cache, reducing memory pressure and compute costs.</p>
</li>
<li>
<p><strong>Cache Sharing:</strong> In advanced configurations, the cache can be shared across pods, enabling even greater efficiency for high-concurrency scenarios.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>The Win:</strong> Every model deployment—whether from your private registry or the public catalog—benefits from intelligent cache management.</p>
</div>
<div class="paragraph">
<p><strong>The Benefit:</strong> You can serve more concurrent users on the same hardware, directly improving your cost-per-token ratio.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_complete_deployment_story"><a class="anchor" href="#_the_complete_deployment_story"></a>The Complete Deployment Story</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This course covers the full lifecycle of intelligent model deployment:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>The Foundation:</strong> Understand how <code>llm-d</code> provides intelligent routing and KV cache management for all model deployments.</p>
</li>
<li>
<p><strong>The Deployment:</strong> Deploy your model using <code>llm-d</code>, whether from your private registry, the public catalog, or Hugging Face.</p>
</li>
<li>
<p><strong>The Payoff:</strong> Your model serves users efficiently, with every GPU cycle optimized for performance and cost.</p>
</li>
</ol>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="title">Prerequisites</div>
<div class="paragraph">
<p>To successfully complete the hands-on sections of this course, you need:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Access to a <strong>Red Hat OpenShift AI 3.0</strong> cluster.</p>
</li>
<li>
<p>A model ready to deploy (from your private registry, the public catalog, or Hugging Face). We will use <code>Qwen/Qwen3-0.6B</code> as an example.</p>
</li>
<li>
<p><code>cluster-admin</code> privileges (to install required operators and configure the Gateway API).</p>
</li>
<li>
<p>The <code>oc</code> CLI tool installed in your terminal.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_your_mission_deploy_with_intelligence"><a class="anchor" href="#_your_mission_deploy_with_intelligence"></a>Your Mission: Deploy with Intelligence</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this course, you will not just deploy a model; you will deploy it <strong>intelligently</strong>. You will take on the role of a Platform Engineer tasked with transforming a registered model into a production-grade inference service.</p>
</div>
<div class="paragraph">
<p><strong>You will execute the following Technical Workflow:</strong></p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>The Architecture:</strong> Understand how <code>llm-d</code> provides intelligent routing and KV cache off-loading for all model deployments.</p>
</li>
<li>
<p><strong>The Foundation:</strong> Verify Gateway API support and install operators (if needed for multi-node or MoE deployments).</p>
</li>
<li>
<p><strong>The Deployment:</strong> Deploy your model using the <code>llm-d</code> runtime, configuring intelligent routing and cache management.</p>
</li>
<li>
<p><strong>The Validation:</strong> Verify that requests are routed intelligently and that KV cache hits reduce latency and cost.</p>
</li>
</ol>
</div>
<hr>
<div class="paragraph">
<p><strong>Ready to deploy? Let&#8217;s start by understanding the architecture.</strong></p>
</div>
</div>
</div>
<nav class="pagination">
  <span class="next"><a href="chapter1/section1.html">Architecture Deep Dive</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../_/js/site.js" data-ui-root-path="../../_"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
