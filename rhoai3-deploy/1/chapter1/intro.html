<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Enterprise Model Serving: The Engineering Reality :: Red Hat OpenShift AI (RHOAI) Enterprise Model Serving</title>
    <link rel="prev" href="#chapter1:section3.adoc">
    <link rel="next" href="gpu-arch.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Red Hat OpenShift AI (RHOAI) Enterprise Model Serving</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai3-deploy" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Red Hat OpenShift AI (RHOAI) Enterprise Model Serving</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Introduction &amp; Value</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="#chapter1:section1.adoc">Architecture Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="#chapter1:section2.adoc">The Deployment Lab</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="#chapter1:section3.adoc">Troubleshooting</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="intro.html">Introduction &amp; Value</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="gpu-arch.html">GPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="vram-calc.html">The Sizing Guide</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="vllm-engine.html">vLLM</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="tuning-vllm.html">vLLM Tuning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="model-serving.html">Deploy a Model</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="summary.html">Testing</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Red Hat OpenShift AI (RHOAI) Enterprise Model Serving</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Red Hat OpenShift AI (RHOAI) Enterprise Model Serving</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Red Hat OpenShift AI (RHOAI) Enterprise Model Serving</a></li>
    <li><a href="intro.html">Introduction &amp; Value</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Enterprise Model Serving: The Engineering Reality</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph lead">
<p><strong>Inference is where the bill arrives.</strong></p>
</div>
<div class="paragraph">
<p>In the experimentation phase, the only metric that matters is "intelligence"—how smart is the model?
But in the production phase, the metrics change. Suddenly, you are judged on <strong>Latency</strong>, <strong>Throughput</strong>, and <strong>Cost Per Token</strong>.</p>
</div>
<div class="paragraph">
<p>A developer might download a 70-billion parameter model to a laptop and be impressed by its reasoning.
A Platform Engineer looks at that same model and sees a massive infrastructure bill, potential <code>OutofMemory</code> errors, and a latency bottleneck that could kill the user experience.</p>
</div>
<div class="paragraph">
<p>This course is your guide to the <strong>Engineering Reality</strong> of AI.
We are moving beyond "Shadow AI"—where users blindly deploy heavy models—to a governed "AI Factory" where inference is fast, efficient, and cost-effective by design.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_core_challenge_will_it_fit"><a class="anchor" href="#_the_core_challenge_will_it_fit"></a>The Core Challenge: "Will It Fit?"</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Deploying an LLM is not like deploying a microservice. You cannot just throw it into a container and hope for the best.
You must answer three critical questions before you ever run a <code>kubectl apply</code>:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>The Selection:</strong> Is this model the right size for the task, or are we burning GPU cycles on overkill?</p>
</li>
<li>
<p><strong>The Hardware:</strong> Do we have enough Video RAM (VRAM) to handle the model weights <strong>plus</strong> the concurrent user traffic (KV Cache)?</p>
</li>
<li>
<p><strong>The Engine:</strong> Is our runtime (vLLM) tuned to maximize throughput, or are we leaving performance on the table?</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_serving_workflow"><a class="anchor" href="#_the_serving_workflow"></a>The Serving Workflow</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this course, we will break down the black box of model serving into a repeatable engineering process.</p>
</div>
<div class="sect2">
<h3 id="_1_smart_model_selection"><a class="anchor" href="#_1_smart_model_selection"></a>1. Smart Model Selection</h3>
<div class="paragraph">
<p>You don&#8217;t have to benchmark 900,000 models from scratch.
We will explore tools like the <strong>Red Hat AI Validated Model Repository</strong>, which acts as a shortcut. It provides pre-vetted models with known performance profiles, allowing you to skip the "guesswork" phase and select models that are guaranteed to work on your hardware.</p>
</div>
</div>
<div class="sect2">
<h3 id="_2_hardware_sizing_the_math"><a class="anchor" href="#_2_hardware_sizing_the_math"></a>2. Hardware Sizing (The Math)</h3>
<div class="paragraph">
<p>We will teach you the formula to calculate the true cost of a model.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Weights vs. Cache:</strong> Understanding why a model that "fits" when idle crashes when under load.</p>
</li>
<li>
<p><strong>Quantization:</strong> How to use formats like <code>FP8</code> or <code>INT8</code> to run massive models on smaller, cheaper GPUs without sacrificing intelligence.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_3_the_engine_vllm_tuning"><a class="anchor" href="#_3_the_engine_vllm_tuning"></a>3. The Engine: vLLM Tuning</h3>
<div class="paragraph">
<p>We will look under the hood of <strong>vLLM</strong>, the standard inference engine for OpenShift AI.
You will learn to tune critical parameters like:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>max-model-len</code>: Managing the context window to prevent memory overflows.</p>
</li>
<li>
<p><code>gpu-memory-utilization</code>: Optimizing VRAM allocation for high-throughput batching.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_4_automated_deployment_gitops"><a class="anchor" href="#_4_automated_deployment_gitops"></a>4. Automated Deployment (GitOps)</h3>
<div class="paragraph">
<p>Finally, we will abandon the "Click-Ops" dashboard.
You will deploy a production-ready <strong>InferenceService</strong> using a parameterized script. This moves your infrastructure from "manual configuration" to "code," enabling reproducible and scalable deployments.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_our_laboratory_model_granite_3_3_2b"><a class="anchor" href="#_our_laboratory_model_granite_3_3_2b"></a>Our Laboratory Model: Granite-3.3-2B</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To practice these concepts without requiring a massive GPU cluster, we will standardize on the <strong>IBM Granite-3.3-2B-Instruct</strong> model.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Why this model?</strong></p>
</li>
<li>
<p><strong>Efficiency:</strong> Its small size (2 Billion parameters) allows us to deploy it quickly and experiment with tuning parameters on standard hardware.</p>
</li>
<li>
<p><strong>Modern Architecture:</strong> It uses the same underlying technology as massive 70B+ models, meaning the sizing and tuning lessons you learn here directly apply to enterprise-scale deployments.</p>
</li>
<li>
<p><strong>Transparency:</strong> It is fully open-source (Apache 2.0) with disclosed training data, representing the gold standard for trusted enterprise AI.</p>
</li>
</ul>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="title">Prerequisites</div>
<div class="paragraph">
<p>To successfully complete the hands-on sections of this course, you need:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Access to a <strong>Red Hat OpenShift AI</strong> cluster.</p>
</li>
<li>
<p>A basic understanding of Kubernetes (Pods, Services, Routes).</p>
</li>
<li>
<p>The <code>oc</code> CLI tool installed in your terminal.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
<hr>
<div class="paragraph">
<p><strong>The goal is efficient inference. Let&#8217;s start by calculating the cost.</strong></p>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="#chapter1:section3.adoc">Troubleshooting</a></span>
  <span class="next"><a href="gpu-arch.html">GPU Architecture</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
