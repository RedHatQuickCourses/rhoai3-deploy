<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Lab: Automated Deployment (GitOps) :: Shadow-Proof AI, The Economics of Governed Serving</title>
    <link rel="prev" href="tuning-vllm.html">
    <link rel="next" href="summary.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Shadow-Proof AI, The Economics of Governed Serving</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/REPLACEREPONAME/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="rhoai3-deploy" data-version="1">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Shadow-Proof AI, The Economics of Governed Serving</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Introduction &amp; Value</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="gpu-arch.html">GPU Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="kserve.html">Kserve Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="vram-calc.html">The Sizing Guide</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="vllm-engine.html">vLLM</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="tuning-vllm.html">vLLM Tuning</a>
  </li>
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="model-serving.html">Deploy a Model</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="summary.html">Summary</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Shadow-Proof AI, The Economics of Governed Serving</span>
    <span class="version">1</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Shadow-Proof AI, The Economics of Governed Serving</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">1</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Shadow-Proof AI, The Economics of Governed Serving</a></li>
    <li><a href="model-serving.html">Deploy a Model</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Lab: Automated Deployment (GitOps)</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph lead">
<p><strong>Click-Ops does not scale. GitOps does.</strong></p>
</div>
<div class="paragraph">
<p>In the previous modules, we selected the <strong>Granite-8B (FP16)</strong> model and calculated the tuning parameters (<code>max-model-len=8000</code>) required to make it run safely on our hardware.</p>
</div>
<div class="paragraph">
<p>Now, we&#8217;re using the <strong>granite-3.3-2b-instruct</strong>, it&#8217;s .75% smaller and faster experience.</p>
</div>
<div class="paragraph">
<p>You <strong>could</strong> open the OpenShift AI Dashboard, click "Add Model Server," copy-paste arguments, and hope you didn&#8217;t make a typo.</p>
</div>
<div class="paragraph">
<p>But what happens when you need to deploy 50 models? Or when you need to update the configuration across three clusters?</p>
</div>
<div class="paragraph">
<p>In this lab, We will deploy our tuned inference stack using a parameterized <strong>Infrastructure-as-Code</strong> scripts.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_fast_track_to_model_deployment_setup"><a class="anchor" href="#_fast_track_to_model_deployment_setup"></a>Fast Track to Model Deployment Setup</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To begin, we need to acquire the automation scripts. We have packaged the entire "inference deployment" into a Git repository.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Open your OpenShift Terminal.</strong></p>
</li>
<li>
<p><strong>Clone the repository:</strong></p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">git clone https://github.com/RedHatQuickCourses/rhoai3-deploy.git</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Switch to the lab directory:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">cd rhoai3-deploy</code></pre>
</div>
</div>
</li>
</ol>
</div>
<hr>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>This utility automates the "grunt work" of the AI Engineer:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Deploys a MinIO Object Store ("The Vault").</p>
</li>
<li>
<p>Creates a Data Connection in OpenShift AI.</p>
</li>
<li>
<p>Downloads a model (<code>granite-3.3-2b-instruct</code>) and places it in the vault (S3 storage).</p>
</li>
<li>
<p>Create a 'fast-track-sa' service account to access the storage.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>This script only works with <strong>Public/Ungated</strong> models (like IBM Granite 4.0 or Qwen 2.5).
It does not support gated models (like Llama 3) that require a Hugging Face Token.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_running_the_script"><a class="anchor" href="#_running_the_script"></a>Running the Script</h2>
<div class="sectionbody">
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Open your OpenShift Terminal.</p>
</li>
<li>
<p>Navigate to the repository root.</p>
</li>
<li>
<p>Execute the script:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">chmod u+x deploy/fast-track.sh
./deploy/fast-track.sh</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="sect2">
<h3 id="_verification"><a class="anchor" href="#_verification"></a>Verification</h3>
<div class="paragraph">
<p>Once the script completes (look for <code>âœ… SUCCESS</code>), verify the environment is ready for serving:</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_model_inference"><a class="anchor" href="#_model_inference"></a>Model Inference</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_step_1_prerequisites"><a class="anchor" href="#_step_1_prerequisites"></a>Step 1: Prerequisites</h3>
<div class="paragraph">
<p>Before proceeding, verify that your "Fast Track" script successfully populated the vault.</p>
</div>
</div>
<div class="sect2">
<h3 id="_check_the_s3_bucket"><a class="anchor" href="#_check_the_s3_bucket"></a><strong>Check the S3 Bucket:</strong></h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Verify the model files exist in the 'models' bucket
oc exec -n model-deploy-lab deployment/minio -- ls /data/models/granite3</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Expected Output:</strong> You should see files like <code>config.json</code>, <code>tokenizer.json</code>, and <code>*.safetensors</code>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_check_the_data_connection"><a class="anchor" href="#_check_the_data_connection"></a><strong>Check the Data Connection:</strong></h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Verify the KServe storage secret exists
oc get secret storage-config -n model-deploy-lab</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_2_the_runtime_the_engine"><a class="anchor" href="#_step_2_the_runtime_the_engine"></a>Step 2: The Runtime (The Engine)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>RHOAI includes a certified vLLM runtime template. Instead of writing one from scratch, we will extract the official template from the cluster and install it in our namespace.</p>
</div>
<div class="sect2">
<h3 id="_extract_and_apply_the_runtime"><a class="anchor" href="#_extract_and_apply_the_runtime"></a><strong>Extract and Apply the Runtime:</strong></h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc process vllm-cuda-runtime-template -n redhat-ods-applications | \
oc apply -f - -n model-deploy-lab</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>To view the runtime config, leave off the:
<strong> | \ oc apply -f - -n model-deploy-lab</strong>, from the previous command.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p><strong>Verify the Runtime:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc get servingruntime vllm-cuda-runtime -n model-deploy-lab</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_create_a_secret_to_access_the_models_files"><a class="anchor" href="#_create_a_secret_to_access_the_models_files"></a>Create a Secret to Access the Models Files</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In order to read the model files from the "vault" storage, we need a storage-config, which is stored as a kubernets secret.
There is supplied file in the deploy folder called dataconn.yaml.  Apply this file to create the secret.</p>
</div>
<div class="listingblock">
<div class="title">dataconn.yaml</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">apiVersion: v1
kind: Secret
metadata:
  name: models-connection  # Renaming for clarity (was 'models' or 'storage-config')
  namespace: model-deploy-lab
  labels:
    # 1. VISIBILITY: Tells the Dashboard "I belong to RHOAI"
    opendatahub.io/dashboard: "true"
    # 2. MANAGEMENT: Tells RHOAI "I am a managed Data Connection"
    opendatahub.io/managed: "true"
  annotations:
    # 3. TYPE: Tells the Injector "I am an S3 credential"
    opendatahub.io/connection-type: s3
    openshift.io/display-name: Models Bucket
type: Opaque
stringData:
  # The keys MUST match standard AWS SDK expectations
  AWS_ACCESS_KEY_ID: minio
  AWS_SECRET_ACCESS_KEY: minio123
  AWS_DEFAULT_REGION: us-east-1
  AWS_S3_ENDPOINT: http://minio-service.model-deploy-lab:9000
  AWS_S3_BUCKET: models</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">deploy the storage secret for use by the deployment</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc apply -f ./deploy/dataconn.yaml</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">links data-connection to fast-track service account</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc secret link fast-track-sa models-connection</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_3_the_deployment_the_workload"><a class="anchor" href="#_step_3_the_deployment_the_workload"></a>Step 3: The Deployment (The Workload)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>We will now create the <code>InferenceService</code>. This Custom Resource binds our <strong>Runtime</strong> (the engine) to our <strong>Storage</strong> (the fuel).</p>
</div>
<div class="sect2">
<h3 id="_create_the_manifest"><a class="anchor" href="#_create_the_manifest"></a><strong>Create the Manifest:</strong></h3>
<div class="paragraph">
<p>Copy the following YAML to a file named <code>granite3-isvc.yaml</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: granite3
  namespace: model-deploy-lab
  annotations:
    # "RawDeployment" mode bypasses Serverless to prevent cold starts
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    serviceAccountName: fast-track-sa
    model:
      # The Format must match what the Runtime supports (vLLM)
      modelFormat:
        name: vLLM
      # This name must match the runtime extracted in Step 2
      runtime: vllm-cuda-runtime
      # The path to your model in MinIO (s3://bucket/folder)
      storageUri: s3://models/granite3
      args:
        - "--max-model-len=4096"        # Limits context to 4k to save VRAM
        - "--gpu-memory-utilization=0.95" # Uses 95% of GPU (leaves 5% for overhead)
       # - "--distributed-executor-backend=mp" # Required for some multi-GPU setups
      resources:
        requests:
          nvidia.com/gpu: "1"
          memory: "8Gi"
        limits:
          nvidia.com/gpu: "1"
          memory: "10Gi"</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_apply_the_deployment"><a class="anchor" href="#_apply_the_deployment"></a><strong>Apply the Deployment:</strong></h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc apply -f ./deploy/deploy.yaml</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_wait_for_readiness"><a class="anchor" href="#_wait_for_readiness"></a><strong>Wait for Readiness:</strong></h3>
<div class="paragraph">
<p>This process can take 2-5 minutes as the vLLM engine initializes and loads the weights into GPU memory.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc wait --for=condition=Ready inferenceservice/granite3 -n model-deploy-lab --timeout=300s</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_4_validate_the_api"><a class="anchor" href="#_step_4_validate_the_api"></a>Step 4: Validate the API</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Once the model is ready, we can test it using the OpenAI-compatible API provided by vLLM.</p>
</div>
<div class="sect2">
<h3 id="_retrieve_the_inference_url"><a class="anchor" href="#_retrieve_the_inference_url"></a><strong>Retrieve the Inference URL:</strong></h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">export MODEL_URL=$(oc get inferenceservice granite3 -n model-deploy-lab -o jsonpath='{.status.url}')
echo "Targeting: $MODEL_URL"</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">curl -k -X POST "${MODEL_URL}:8080/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "granite3",
    "messages": [{"role": "user", "content": "Write a Poem about AI"}]
  }'</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Expected Output:</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">{
  "id": "cmpl-...",
  "object": "chat.completion",
  "created": 1700000000,
  "model": "granite3",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Pods rise in the cloud,\nGPUs hum with quiet power,\nCode serves thought to all."
      },
      "finish_reason": "stop"
    }
  ]
}</code></pre>
</div>
</div>
<hr>
</div>
<div class="sect2">
<h3 id="_thats_a_wrap"><a class="anchor" href="#_thats_a_wrap"></a>That&#8217;s a Wrap</h3>
<div class="paragraph">
<p>This is one example of model inference using the built-in runtime from the command line.</p>
</div>
<div class="paragraph">
<p>Based on your feedback, we&#8217;ll update this course accordingly:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>What type of model inference examples should be add to this course?</p>
</li>
<li>
<p>What additional features need to be supported such as tool calling templates?</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Let us know by <a href="https://github.com/RedHatQuickCourses/rhoai3-deploy/issues" target="blank">creating an issue in the course repository</a></p>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="tuning-vllm.html">vLLM Tuning</a></span>
  <span class="next"><a href="summary.html">Summary</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
